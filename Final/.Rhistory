psi_b_ia = 30.2 - 0.5*24.2 - 0.5*27.1
s_psi = sqrt(MS_SA*1.5/n)
t_b_ia = psi_b_ia/s_psi
S = sqrt(2 * 2.69)
t_b_ia > S
S = sqrt(2 * 3.35) #Scheffe's method and appendix C.5
t_b_ia > S #True. Reject the null
rm(list = ls())
#Read data into relevant variables
data = read.csv(file = file.choose())
cell_means = matrix(c(12, 16, 14, 18, 14, 12), byrow = TRUE)
cell_means
cell_means = matrix(c(12, 16, 14, 18, 14, 12), nrow = 2, byrow = TRUE)
cell_means
a = 2
b = 3
n = 6
A_means = rowMeans(cell_means)
grand_mean = mean(cell_means)
B_means = colMeans(cell_means)
SS_cells = n*(sum(cell_means - grand_mean)^2)
SS_A = n*b*(sum(A_means - grand_mean)^2)
SS_B = n*a*(sum(B_means - grand_mean)^2)
SSAB = SS_cells - SS_A - SS_B
SS_A = n*b*(sum(B_means - grand_mean)^2)
SS_B = n*a*(sum(A_means - grand_mean)^2)
SSAB = SS_cells - SS_A - SS_B
SS_A = n*b*(sum(B_means - grand_mean)^2)
SS_B = n*a*(sum(A_means - grand_mean)^2)
SSAB = SS_cells - SS_A - SS_B
SS_A = n*b*(sum(A_means - grand_mean)^2)
A_means - grand_mean
SS_A = n*b*(sum((A_means - grand_mean)^2))
SS_B = n*a*(sum((B_means - grand_mean)^2))
SSAB = SS_cells - SS_A - SS_B
SS_cells = n*(sum((cell_means - grand_mean)^2))
SS_A = n*b*(sum((A_means - grand_mean)^2))
SS_B = n*a*(sum((B_means - grand_mean)^2))
SSAB = SS_cells - SS_A - SS_B
dfA = a - 1
dfB = b - 1
dfAB = (a-1)*(b-1_)
dfAB = (a-1)*(b-1)
MSA = SS_A/dfA
MSB = SS_B/dfB
MSAB = SS_AB/dfAB
MSAB = SSAB/dfAB
F = MSA/MSAB
FA = MSA/MSAB
FB = MSB/MSAB
MS_SAB = MSAB/FAB
FAB = 8
MS_SAB = MSAB/FAB
dfSAB = a*b*(n-1)
SS_SAB = dfSAB*MS_SAB
SS_total = SS_SAB + SS_cells
FA = MSA/MS_SAB
FB = MSB/MSS_AB
FB = MSB/MS_SAB
pf(FA, dfA, dfSAB, lower.tail = F)
pf(FB, dfB, dfSAB, lower.tail = F)
pf(FAB, dfAB, dfSAB, lower.tail = F)
B_means
SSBA2 = n * 1 * sum(((B_means - B_means[2])^2))
SSBA2 = n * 1 * sum(((A_means - B_means[2])^2))
SSBA2 = n * 1 * sum(((B_means - A_means[2])^2))
SSBA2 = n * 1 * sum(((A_means - B_means[2])^2))
mean(18, 14, 12)
mean(c(18, 14, 12))
A_means
cell_means[2,]
SSBA2 = sum((A_means - cell_means[2, ])^2)
SSBA2 = n*sum((A_means - cell_means[2, ])^2)
SSBA2 = n*sum((A_means - cell_means[2, ])^2)
SSBA2 = n*sum((A_means[2] - cell_means[2, ])^2)
MSBA2 = SSBA2/2
FBA2 = MSBA2/MS_SAB
pf(FBA2, 2, dfSAB)
pf(FBA2, 2, dfSAB, lower.tail = F)
TukeyHSD(aov.mult)
rm(list = ls())
#Read data into relevant variables
data = read.csv(file = file.choose())
distance = data['x11']
gender = data['x7']
recommend = data['x8']
#####################################################
female_low_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 1, 1]
female_mid_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 2, 1]
female_high_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 3, 1]
male_low_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 1, 1]
male_mid_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 2, 1]
male_high_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 3, 1]
#Testing for normality
par(mfrow=c(2, 3), row.names(c("males", "females")), colnames(c('low, mid, high')))
#Testing for normality
par(mfrow=c(2, 3), row.names(c("males", "females")), colnames(c('low, mid, high')))
qqnorm(male_low_recommend, main = "Recommendations", col = 'blue')
qqnorm(male_mid_recommend, main = "Recommendations", col = 'blue')
qqnorm(male_high_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_low_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_high_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_mid_recommend, main = "Recommendations", col = 'blue')
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
hist(male_mid_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
hist(female_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
hist(male_high_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
hist(female_mid_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
hist(female_high_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue')
#hist(recommend[[1]], main = "Recommendations", col = "blue", breaks = seq(1, 7, 1))
#barplot(table(recommend[[1]]), ylab = "Frequencies", main = "Recommendations", xlab = "Scores", col = "blue")
library('e1071')
skewness(recommend[[1]])
shapiro.test(male_mid_recommend)
#data:  male_mid_recommend
#W = 0.42208, p-value = 4.195e-08
shapiro.test(male_low_recommend)
#data:  male_low_recommend
#W = 0.41679, p-value = 1.109e-12
shapiro.test(female_high_recommend)
#data:  female_high_recommend
#W = 0.63036, p-value = 6.75e-07
shapiro.test(female_mid_recommend)
shapiro.test(female_low_recommend)
library(car)
leveneTest(recommend[[1]], group = as.factor(gender[[1]]))
leveneTest(recommend[[1]], group = as.factor(distance[[1]]))
gender_fac <- as.factor(gender[[1]])
dist_fac <- as.factor(distance[[1]])
sub <- as.factor(1:200)
d <- data.frame(recommend = recommend[[1]], gender_fac = gender_fac, sub = sub, dist_fac = dist_fac)
aov.mult <- aov(recommend ~ gender_fac*dist_fac, data = d)
Method <- as.factor(gender_fac*dist_fac)
summary(aov.mult)
library(multcomp)
TukeyHSD(aov.mult)
TukeyHSD(aov.mult, ordered = T)
TukeyHSD(aov.mult)
install.packages('car')
install.packages("car")
install.packages('ez')
library(ez)
ez_anova = ezANOVA(data = d, dv = .(recommend[[1]], wid = .(sub), between = .(gender_fac, dist_fac), detailed = TRUE))
ez_anova = ezANOVA(data = d, dv = .(recommend[[1]]), wid = .(sub), between = .(gender_fac, dist_fac), detailed = TRUE))
ez_anova = ezANOVA(data = d, dv = .(recommend[[1]]), wid = .(sub), between = .(gender_fac, dist_fac), detailed = TRUE)
ez_anova = ezANOVA(data = d, dv = .(recommend = recommend[[1]]), wid = .(sub), between = .(gender_fac, dist_fac), detailed = TRUE)
ez_anova = ezANOVA(data = d, dv = .(recommend), wid = .(sub), between = .(gender_fac, dist_fac), detailed = TRUE)
ez_anova = ezANOVA(data = d, dv = .(recommend), wid = .(sub), between = .(dist_fac, gender_fac), detailed = TRUE)
summary(ez_anova)
ez_anova <- ezANOVA(data = d, dv = .(recommend), wid = .(sub), between = .(dist_fac, gender_fac), detailed = TRUE)
summary(ez_anova)
ez_anova
p = c(1/3, 1/3, 1/3)
n_trials = 100
alpha = 0.5
data = read.csv(file = file.choose())
a_to_transition <- function(data){
a = rev(data$Dist)
a_trans = a/sum(a)
a_trans = a_trans * (1/3) / a_trans[n_trials]
a_trans[a_trans>1]=1
return (a_trans)
}
observed_transition_prob = a_to_transition(data)
rm(list = ls())
p = c(1/3, 1/3, 1/3)
p = 1/3
n_trials = 100
alpha = 0.5
data = read.csv(file = file.choose())
a_to_transition <- function(data){
a = rev(data$Dist)
a_trans = a/sum(a)
a_trans = a_trans * (1/3) / a_trans[n_trials]
a_trans[a_trans>1]=1
return (a_trans)
}
observed_transition_prob = a_to_transition(data)
observed_transition_prob
pred_trans = function(alpha){
p = c(1/3)
for (i in (1:n_trials)){
p = c(p, p[i] + ((n_trials/100) - i/100) * alpha)
}
return(p)
}
predicted_transition_prob = pred_trans(0.01)
predicted_transition_prob
chi_sq = function(o, p){
return (sum((o-e)^2/e))
}
chi_sq(observed_transition_prob, predicted_transition_prob)
chi_sq = function(o, p){
return (sum((o-p)^2/p))
}
chi_sq(observed_transition_prob, predicted_transition_prob)
length(observed_transition_prob)
length(predicted_transition_prob)
pred_trans = function(alpha){
p = c(1/3)
for (i in (1:n_trials-1)){
p = c(p, p[i] + ((n_trials/100) - i/100) * alpha)
}
return(p)
}
chi_sq = function(o, p){
return (sum((o-p)^2/p))
}
predicted_transition_prob = pred_trans(0.01)
chi_sq(observed_transition_prob, predicted_transition_prob)
linear_search = function(range, step){
alpha = 0.001
min_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(alpha))
for (i in seq(from = min(range), to = max(range), by = 0.0001)){
new_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(i))
if (new_chi_sq_val < min_chi_sq_val){
min_chi_sq_val = new_chi_sq_val
}
}
return (alpha)
}
linear_search = function(range, step){
alpha = 0.001
min_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(alpha))
for (i in seq(from = min(range), to = max(range), by = 0.0001)){
new_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(i))
if (new_chi_sq_val < min_chi_sq_val){
min_chi_sq_val = new_chi_sq_val
alpha = i
}
}
return (alpha)
}
linear_search(c(0.0001, 0.1))
linear_search(c(0.0001, 0.1))
alpha = linear_search(c(0.0001, 0.1))
p = pred_trans(alpha)
p
observed_transition_prob
rm(list = ls())
#Read data into relevant variables
data = read.csv(file = file.choose())
distance = data['x11']
gender = data['x7']
recommend = data['x8']
#####################################################
female_low_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 1, 1]
female_mid_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 2, 1]
female_high_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 3, 1]
male_low_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 1, 1]
male_mid_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 2, 1]
male_high_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 3, 1]
#Testing for normality
par(mfrow=c(2, 3), row.names(c("males", "females")), colnames(c('low, mid, high')))
qqnorm(male_low_recommend, main = "Recommendations", col = 'blue')
qqnorm(male_mid_recommend, main = "Recommendations", col = 'blue')
qqnorm(male_high_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_low_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_mid_recommend, main = "Recommendations", col = 'blue')
qqnorm(female_high_recommend, main = "Recommendations", col = 'blue')
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = 40)
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = "40")
?hist
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = (1:40))
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40)
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40)
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
#Testing for normality
par(mfrow=c(2, 3), row.names(c("males", "females")), colnames(c('low, mid, high')))
hist(male_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
hist(male_mid_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
hist(male_high_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
hist(female_low_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
hist(female_mid_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
hist(female_high_recommend, main = "Recommendations", breaks = seq(1, 7, 1), col = 'blue', ylim = c(1, 40))
?aov
library(ez)
?ezANOVA
shapiro.test(recommend[[1]])
rm(list = ls())
#Read data into relevant variables
data = read.csv(file = file.choose())
distance = data['x11']
gender = data['x7']
recommend = data['x8']
#####################################################
female_low_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 1, 1]
female_mid_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 2, 1]
female_high_recommend = recommend[gender[[1]] == 0 & distance[[1]] == 3, 1]
male_low_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 1, 1]
male_mid_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 2, 1]
male_high_recommend = recommend[gender[[1]] == 1 & distance[[1]] == 3, 1]
#Testing for normality
par(mfrow=c(2, 3), row.names(c("males", "females")), colnames(c('low, mid, high')))
#hist(recommend[[1]], main = "Recommendations", col = "blue", breaks = seq(1, 7, 1))
#barplot(table(recommend[[1]]), ylab = "Frequencies", main = "Recommendations", xlab = "Scores", col = "blue")
library('e1071')
skewness(recommend[[1]])
shapiro.test(recommend[[1]])
rm(list = ls())
p = 1/3
n_trials = 100
alpha = 0.5
data = read.csv(file = file.choose())
a_to_transition <- function(data){
a = rev(data$Dist)
a_trans = a/sum(a)
a_trans = a_trans * (1/3) / a_trans[n_trials]
a_trans[a_trans>1]=1
return (a_trans)
}
observed_transition_prob = a_to_transition(data)
pred_trans = function(alpha){
p = c(1/3)
for (i in (1:n_trials-1)){
p = c(p, p[i] + ((n_trials/100) - i/100) * alpha)
}
return(p)
}
chi_sq = function(o, p){
return (sum((o-p)^2/p))
}
predicted_transition_prob = pred_trans(0.01)
chi_sq(observed_transition_prob, predicted_transition_prob)
linear_search = function(range){
alpha = 0.001
min_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(alpha))
for (i in seq(from = min(range), to = max(range), by = 0.0001)){
new_chi_sq_val = chi_sq(observed_transition_prob, pred_trans(i))
if (new_chi_sq_val < min_chi_sq_val){
min_chi_sq_val = new_chi_sq_val
alpha = i
}
}
return (alpha)
}
alpha = linear_search(c(0.0001, 0.1))
p = pred_trans(alpha)
observed_transition_prob
alpha
min(data$Dist) * 3 * p
predicted_dist = rev(min(data$Dist)*p/(1/3))
predicted_dist
data[,2]
predicted_dist = min(data$Dist)*p/(1/3)
predicted_dist
data[,a]
data[,2]
write.csv(predicted_dist, file = "Predicted_start_points.csv")
setwd("~/Courses/Psych 891/Psych891/Final")
write.csv(predicted_dist, file = "Predicted_start_points.csv")
data = read.csv(file = "Predicted_start_points.csv")
head(data)
#quantile(total_rts[,1], c(0.1, 0.3, 0.5, 0.7, 0.9))
source("wald.R")
source("TruncNorm.R")
#RTs during each trial. Where a trial is a sequence of length 12. So there'd be 12 RTs.
#Assume 100 trials
n_trials = 100
v = rtnorm(n_trials, mu = 0.3, sig = 0.1, lo = 0.1, hi = 0.5)
trial_rt = c()
#12 RTs generated from wald distribution for a given starting point and sampled drift rate.
#Quantiles are returned and put in a list. Unpacked in a matrix in the line that follows.
#
for (i in 1:n_trials){
trial_rt = c(trial_rt, waldSimq(data[,2][n_trials - i + 1]/100, v[i], 12))
}
trial_rt_quantiles = matrix(trial_rt, nrow = n_trials, byrow = TRUE)
#Expected frequencies
f = c(0.1, 0.2, 0.2, 0.2, 0.2, 0.1)*12
#Fit parameters to save
f_v = c()
f_wald_gsq = c()
cnt = 0
upfreq = 50
waldPred <- function(a, par, q){
#Parameters
#a = par[1]
v = par[1]
#print (a)
#print (v)
#Cuts for quantiles
#print (q)
cuts = c(0, q, Inf)
p = c()
for (i in (1:(length(cuts)-1))){
p[i] = pwald(cuts[i+1], a, v) - pwald(cuts[i], a, v)
#print (pwald(cuts[i], a, v))
}
return(p)
}
gsqfun_wald <- function(par, a, q, freq){
#ls = cons(par)
#cpar = ls[[1]]
#pen = ls[[2]]
#p = waldPred(a, cpar, q)
p = waldPred(a, par, q)
#print (p)
p[p<.0001]=.0001
p = p/sum(p)
#print (p)
pf = p*sum(freq)
gslev=1:length(freq)
gslev=gslev[freq>0]
gsq=2*sum(freq[gslev]*log(freq[gslev]/pf[gslev]))
#gsq = gsq + pen*gsq
return (gsq)
}
linear_search = function(boundary, a, q, f){
par = min(boundary)
min_gsq = gsqfun_wald(par, a, q, f)
#print ("here")
for (i in seq(from = min(boundary), to = max(boundary), by = 0.01)){
gsq = gsqfun_wald(i, a, q, f)
#print (gsq)
if (gsq < min_gsq){
min_gsq = gsq
par = i
}
}
return(par)
}
#linear_search(c(0.01, 0.5), a, q_wald, f_wald)
wald_fit = function(a, q, f){
#par = c(0.11)
#comp = gsqfun_wald(par, a, q, f)
#  while(TRUE){
#    fit = optimize(gsqfun_wald, interval = c(0, 10), a, q, f)
#    new_param = bisect(par, bound)
#    fit=optim(par,gsqfun_wald,a = a, freq=f,q=q)
#    par=fit$par
#    if((comp-fit$value)<=.001) break
#    comp=fit$value
#  }
par = linear_search(c(0.01, 1), a, q, f)
wald_par = par
gsq = gsqfun_wald(a, wald_par, q, f)
ls = list(wald_par, gsq)
return (ls)
}
si= 1
cnt = 0
upfreq = 20
#Run loops to fit the drift rate for each trial (12 subtrials)
for(si in 1:n_trials){
q_wald = trial_rt_quantiles[si,]
a = data[,2][si]/100
#runs the sim function above
#q_wald=waldSimq(a[si], v[si], N)
f_wald = f
#Creates a vector of observed frequency
#  counts in each RT bin separated by
#  quantiles from the exgsim function
#print (si)
ls=wald_fit(a, q_wald, f_wald)
#return fit parameters and gsq values.
fpar=ls[[1]]
gval=ls[[2]]
#  f_a[si]=fpar[1]
f_v[si]=fpar[1]
f_wald_gsq[si]=gval
#show progress updates
if(cnt%%upfreq==0) print(cnt)
cnt=cnt+1
}
f_wald_gsq
f_v
#linear_search(c(0.01, 0.5), a, q_wald, f_wald)
wald_fit = function(a, q, f){
#par = c(0.11)
#comp = gsqfun_wald(par, a, q, f)
#  while(TRUE){
#    fit = optimize(gsqfun_wald, interval = c(0, 10), a, q, f)
#    new_param = bisect(par, bound)
#    fit=optim(par,gsqfun_wald,a = a, freq=f,q=q)
#    par=fit$par
#    if((comp-fit$value)<=.001) break
#    comp=fit$value
#  }
par = linear_search(c(0.01, 5), a, q, f)
wald_par = par
gsq = gsqfun_wald(a, wald_par, q, f)
ls = list(wald_par, gsq)
return (ls)
}
si= 1
cnt = 0
upfreq = 20
#Run loops to fit the drift rate for each trial (12 subtrials)
for(si in 1:n_trials){
q_wald = trial_rt_quantiles[si,]
a = data[,2][si]/100
#runs the sim function above
#q_wald=waldSimq(a[si], v[si], N)
f_wald = f
#Creates a vector of observed frequency
#  counts in each RT bin separated by
#  quantiles from the exgsim function
#print (si)
ls=wald_fit(a, q_wald, f_wald)
#return fit parameters and gsq values.
fpar=ls[[1]]
gval=ls[[2]]
#  f_a[si]=fpar[1]
f_v[si]=fpar[1]
f_wald_gsq[si]=gval
#show progress updates
if(cnt%%upfreq==0) print(cnt)
cnt=cnt+1
}
f_wald_gsq
f_v
